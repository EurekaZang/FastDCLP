{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb23b414",
   "metadata": {},
   "source": [
    "# FastTD3 Model Evaluation with PlayIsaacLabEnv\n",
    "\n",
    "This notebook demonstrates how to use the PlayIsaacLabEnv class to evaluate and visualize trained FastTD3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append('..')\n",
    "\n",
    "from fast_td3.fast_td3 import Actor, MultiTaskActor\n",
    "from fast_td3.environments.play_isaaclab_env import PlayIsaacLabEnv\n",
    "from fast_td3.fast_td3_utils import EmpiricalNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f9b65",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc8e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = \"../models/your_trained_model.pt\"  # Update with your model path\n",
    "TASK_NAME = \"Isaac-Cartpole-v0\"  # Update with your task name\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_EPISODES = 5\n",
    "SEED = 42\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Model path: {MODEL_PATH}\")\n",
    "print(f\"Task: {TASK_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a101b9",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_checkpoint(model_path: str, device: str):\n",
    "    \"\"\"Load model checkpoint and extract components.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model not found: {model_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    print(\"Checkpoint keys:\", list(checkpoint.keys()))\n",
    "    \n",
    "    # Extract actor\n",
    "    actor_state_dict = checkpoint.get('actor_state_dict', checkpoint.get('actor'))\n",
    "    if actor_state_dict is None:\n",
    "        print(\"Warning: Could not find actor in checkpoint\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # You'll need to manually specify the actor parameters\n",
    "    # These should match what was used during training\n",
    "    actor_kwargs = {\n",
    "        'n_obs': 1090,  # Update based on your observation space\n",
    "        'n_act': 2,     # Update based on your action space  \n",
    "        'num_envs': 1,\n",
    "        'init_scale': 0.1,\n",
    "        'hidden_dim': 256,\n",
    "        'device': device,\n",
    "    }\n",
    "    \n",
    "    actor = Actor(**actor_kwargs)\n",
    "    actor.load_state_dict(actor_state_dict)\n",
    "    actor.to(device)\n",
    "    actor.eval()\n",
    "    \n",
    "    # Load normalizers if available\n",
    "    obs_normalizer = None\n",
    "    if 'obs_normalizer' in checkpoint:\n",
    "        obs_normalizer = EmpiricalNormalization(shape=actor_kwargs['n_obs'], device=device)\n",
    "        obs_normalizer.load_state_dict(checkpoint['obs_normalizer'])\n",
    "        obs_normalizer.eval()\n",
    "    \n",
    "    return actor, obs_normalizer, checkpoint\n",
    "\n",
    "# Load the model\n",
    "actor, obs_normalizer, checkpoint = load_model_checkpoint(MODEL_PATH, DEVICE)\n",
    "\n",
    "if actor is not None:\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"Actor type: {type(actor).__name__}\")\n",
    "    if obs_normalizer is not None:\n",
    "        print(\"‚úÖ Observation normalizer loaded\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b825293",
   "metadata": {},
   "source": [
    "## Create Play Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3507c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the play environment\n",
    "play_env = PlayIsaacLabEnv(\n",
    "    task_name=TASK_NAME,\n",
    "    device=DEVICE,\n",
    "    num_envs=1,\n",
    "    seed=SEED,\n",
    "    enable_viewport=True,  # Set to False if running headless\n",
    "    record_video=True,\n",
    "    video_path=\"./evaluation_videos/\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Environment created successfully\")\n",
    "print(f\"Observation space: {play_env.num_obs}\")\n",
    "print(f\"Action space: {play_env.num_actions}\")\n",
    "print(f\"Max episode steps: {play_env.max_episode_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09183956",
   "metadata": {},
   "source": [
    "## Single Episode Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a74e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a single episode\n",
    "if actor is not None:\n",
    "    print(\"Playing single episode...\")\n",
    "    \n",
    "    episode_stats = play_env.play_episode(\n",
    "        actor_model=actor,\n",
    "        deterministic=True,\n",
    "        obs_normalizer=obs_normalizer,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìä Episode Results:\")\n",
    "    for key, value in episode_stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"‚ùå No actor model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967951ea",
   "metadata": {},
   "source": [
    "## Multi-Episode Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a1f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate over multiple episodes\n",
    "if actor is not None:\n",
    "    print(f\"Evaluating over {NUM_EPISODES} episodes...\")\n",
    "    \n",
    "    results = play_env.evaluate_model(\n",
    "        actor_model=actor,\n",
    "        num_episodes=NUM_EPISODES,\n",
    "        obs_normalizer=obs_normalizer,\n",
    "        deterministic=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìà Evaluation completed!\")\n",
    "else:\n",
    "    print(\"‚ùå No actor model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069bd367",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536121b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the evaluation results\n",
    "if actor is not None and 'results' in locals():\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Episode rewards\n",
    "    ax1.bar(range(len(results['episode_rewards'])), results['episode_rewards'])\n",
    "    ax1.axhline(y=results['mean_reward'], color='r', linestyle='--', \n",
    "                label=f'Mean: {results[\"mean_reward\"]:.2f}')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.set_title('Episode Rewards')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Episode lengths\n",
    "    ax2.bar(range(len(results['episode_lengths'])), results['episode_lengths'])\n",
    "    ax2.axhline(y=results['mean_length'], color='r', linestyle='--',\n",
    "                label=f'Mean: {results[\"mean_length\"]:.1f}')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Length')\n",
    "    ax2.set_title('Episode Lengths')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nüìä Summary Statistics:\")\n",
    "    print(f\"Mean Reward: {results['mean_reward']:.2f} ¬± {results['std_reward']:.2f}\")\n",
    "    print(f\"Mean Length: {results['mean_length']:.1f} ¬± {results['std_length']:.1f}\")\n",
    "    print(f\"Success Rate: {results['success_rate']:.2%}\")\n",
    "else:\n",
    "    print(\"‚ùå No results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f460f0",
   "metadata": {},
   "source": [
    "## Compare with Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b6e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random policy for comparison\n",
    "class RandomPolicy:\n",
    "    def __init__(self, action_dim, device):\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "    \n",
    "    def explore(self, obs, deterministic=False):\n",
    "        batch_size = obs.shape[0]\n",
    "        return torch.randn(batch_size, self.action_dim, device=self.device)\n",
    "\n",
    "# Evaluate random policy\n",
    "print(\"Evaluating random policy for comparison...\")\n",
    "random_policy = RandomPolicy(play_env.num_actions, DEVICE)\n",
    "\n",
    "random_results = play_env.evaluate_model(\n",
    "    actor_model=random_policy,\n",
    "    num_episodes=NUM_EPISODES,\n",
    "    deterministic=False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Compare results\n",
    "if actor is not None and 'results' in locals():\n",
    "    print(\"\\nüîç Trained vs Random Policy Comparison:\")\n",
    "    print(f\"Trained Policy - Mean Reward: {results['mean_reward']:.2f}\")\n",
    "    print(f\"Random Policy  - Mean Reward: {random_results['mean_reward']:.2f}\")\n",
    "    print(f\"Improvement: {results['mean_reward'] - random_results['mean_reward']:.2f}\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    policies = ['Trained', 'Random']\n",
    "    rewards = [results['mean_reward'], random_results['mean_reward']]\n",
    "    errors = [results['std_reward'], random_results['std_reward']]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bars = plt.bar(policies, rewards, yerr=errors, capsize=5, \n",
    "                   color=['green', 'red'], alpha=0.7)\n",
    "    plt.ylabel('Mean Episode Reward')\n",
    "    plt.title('Trained vs Random Policy Performance')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, reward in zip(bars, rewards):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{reward:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ùå Cannot compare - trained model results not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58492364",
   "metadata": {},
   "source": [
    "## Environment Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccb63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get environment statistics\n",
    "env_stats = play_env.get_statistics()\n",
    "print(\"\\nüèÜ Environment Statistics:\")\n",
    "for key, value in env_stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e48ff",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a24d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (if needed)\n",
    "print(\"\\nüßπ Evaluation completed!\")\n",
    "if hasattr(play_env, 'video_frames'):\n",
    "    print(f\"Videos saved to: {play_env.video_path}\")\n",
    "print(\"You can now close the Isaac Lab simulator window.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
